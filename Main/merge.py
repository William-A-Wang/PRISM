# -*- coding: utf-8 -*-
"""merge.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13F3hnvBvpcLiUZZH633h-gYrWItjNp1c
"""

pip install Bio

pip install primer3-py

from Bio import SeqIO
import os
from io import StringIO
import random
import primer3
import numpy as np
import pandas as pd
from numba import njit, prange
import matplotlib.pyplot as plt
from typing import List, Tuple, Set, Dict

def sliding_window_regions(sequence: str, window_sizes: List[int], overlaps: int = 250) -> Dict[int, List[Dict]]:
    all_regions = {}
    sequence_length = len(sequence)

    for window_size in window_sizes:
        step_size = window_size - overlaps
        start = 0
        regions = []

        while start < sequence_length:
            end = min(start + window_size, sequence_length)
            region = {
                'id': f'size_{window_size}_region_{start}_{end}',
                'sequence': sequence[start:end],
                'start': start,
                'end': end,
                'length': end - start
            }
            regions.append(region)
            start += step_size

        all_regions[window_size] = regions

    return all_regions

def design_primers(regions: dict) -> Dict[str, List[dict]]:

    primer_results = {}

    global_primer3_settings = {
        'PRIMER_TASK': 'generic',
        'PRIMER_PICK_LEFT_PRIMER': 1,
        'PRIMER_PICK_RIGHT_PRIMER': 1,
        'PRIMER_NUM_RETURN': 5,
        'PRIMER_MIN_SIZE': 20,
        'PRIMER_MAX_SIZE': 26,
        'PRIMER_MIN_TM': 52.0,
        'PRIMER_MAX_TM': 62.0,
        'PRIMER_MIN_GC': 40.0,
        'PRIMER_MAX_GC': 60.0
    }

    for window_size, region_list in regions.items():
        for region in region_list:
            seq_args = {
                'SEQUENCE_ID': region['id'],
                'SEQUENCE_TEMPLATE': region['sequence'],
                'SEQUENCE_INCLUDED_REGION': [0, len(region['sequence'])]
            }

            try:
                result = primer3.bindings.designPrimers(seq_args, global_primer3_settings)
                primer_pairs = []

                num_pairs = result.get('PRIMER_PAIR_NUM_RETURNED', 0)

                for i in range(num_pairs):
                    primer_info = {
                        'pair_id': f"{region['id']}_pair_{i}",
                        'left_primer': {
                            'sequence': result[f'PRIMER_LEFT_{i}_SEQUENCE'],
                            'tm': result[f'PRIMER_LEFT_{i}_TM'],
                            'gc_percent': result[f'PRIMER_LEFT_{i}_GC_PERCENT'],
                            'position': result[f'PRIMER_LEFT_{i}'][0],
                            'length': result[f'PRIMER_LEFT_{i}'][1]
                        },
                        'right_primer': {
                            'sequence': result[f'PRIMER_RIGHT_{i}_SEQUENCE'],
                            'tm': result[f'PRIMER_RIGHT_{i}_TM'],
                            'gc_percent': result[f'PRIMER_RIGHT_{i}_GC_PERCENT'],
                            'position': result[f'PRIMER_RIGHT_{i}'][0],
                            'length': result[f'PRIMER_RIGHT_{i}'][1]
                        },
                        'product_size': result[f'PRIMER_PAIR_{i}_PRODUCT_SIZE']
                    }
                    primer_pairs.append(primer_info)

                primer_results[region['id']] = primer_pairs


                print(f"\n=== Primer Design for {region['id']} ===")
                if num_pairs > 0:
                    for primer in primer_pairs:
                        print(f"  Primer Pair {primer['pair_id']}:")
                        print(f"    Left: {primer['left_primer']['sequence']} (Tm: {primer['left_primer']['tm']:.1f}°C, GC: {primer['left_primer']['gc_percent']:.1f}%)")
                        print(f"    Right: {primer['right_primer']['sequence']} (Tm: {primer['right_primer']['tm']:.1f}°C, GC: {primer['right_primer']['gc_percent']:.1f}%)")
                        print(f"    Product Size: {primer['product_size']} bp")
                else:
                    print("  No primers found.")

            except Exception as e:
                print(f"Error in designing primers for {region['id']}: {str(e)}")

    return primer_results

def clean_sequence(sequence: str) -> str:
    """Clean the fasta sequence"""
    if sequence.startswith('>'):
        sequence = '\n'.join(sequence.split('\n')[1:])
    sequence = ''.join(sequence.split())
    return sequence

def process_consensus_sequence(sequence: str,
                             window_sizes: list = [1000, 2000, 3000],
                             overlaps: int = 250) -> dict:


    clean_seq = clean_sequence(sequence)
    print(f"Cleaned sequence length: {len(clean_seq)} bp")

    print(f"\nSliding window segmentation with overlap {overlaps}bp...")
    regions = sliding_window_regions(clean_seq, window_sizes, overlaps)
    print(regions)

    print("\nDesigning primers using Primer3...")
    primer_results = design_primers(regions)


    print("\nPrimer design statistics:")
    for window_size, region_list in regions.items():
        print(f"\nWindow size {window_size}bp:")
        success_count = sum(1 for region in region_list if len(primer_results.get(region['id'], [])) > 0)
        print(f"Regions with successful primer designs: {success_count}/{len(region_list)}")


        positions = [(r['start'], r['end']) for r in region_list]
        gaps = [positions[i+1][0] - positions[i][1] for i in range(len(positions)-1)]
        if gaps:
            avg_gap = sum(gaps) / len(gaps)
            min_gap_found = min(gaps)
            print(f"Average gap between regions: {avg_gap:.1f}bp")
            print(f"Minimum gap between regions: {min_gap_found}bp")

    return primer_results

def read_consensus_sequence(file_path):
    with open(file_path, 'r') as file:
        lines = file.readlines()[1:]

        consensus_sequence = "".join([line.strip() for line in lines])

    return consensus_sequence

if __name__ == "__main__":
    file_path = "/content/drive/MyDrive/ConsensusSequence.fasta"

    try:

        sequence = read_consensus_sequence(file_path)

        primer_results = process_consensus_sequence(
            sequence,
            window_sizes=[1000],
            overlaps=250
        )


        print("\nPrimer Design Completed. Total Regions Processed:", len(primer_results))

    except Exception as e:
        print(f"Error occurred: {str(e)}")

print(primer_results)

@njit
def is_complementary_numba(base1: int, base2: int) -> bool:
    """Check if two nucleotides are complementary using ASCII values"""
    return ((base1 == 65 and base2 == 84) or  # A-T
            (base1 == 84 and base2 == 65) or  # T-A
            (base1 == 67 and base2 == 71) or  # C-G
            (base1 == 71 and base2 == 67))    # G-C

@njit
def is_same_region(region1: np.array, region2: np.array) -> bool:
    """Check if two regions represent the same complementary sequence
    (either directly or reversed)"""
    # Check if regions are identical in both directions
    direct_match = (region1[0] == region2[0] and
                   region1[1] == region2[1] and
                   region1[2] == region2[2] and
                   region1[3] == region2[3])

    reverse_match = (region1[0] == region2[2] and
                    region1[1] == region2[3] and
                    region1[2] == region2[0] and
                    region1[3] == region2[1])

    return direct_match or reverse_match

@njit
def find_complementary_regions_numba(primer1: np.array, primer2: np.array, min_len: int = 4):
    """Find all complementary regions between two primers using Numba.
    Returns a 2D array where each row contains [start1, end1, start2, end2, length]
    """
    m, n = len(primer1), len(primer2)
    dp = np.zeros((m + 1, n + 1), dtype=np.int32)

    # Pre-allocate maximum possible size for results
    max_possible_results = m * n
    all_results = np.zeros((max_possible_results, 5), dtype=np.int32)
    result_count = 0

    # First pass: find all complementary regions
    for i in range(1, m + 1):
        for j in range(1, n + 1):
            if is_complementary_numba(primer1[i-1], primer2[j-1]):
                dp[i, j] = dp[i-1, j-1] + 1
                if dp[i, j] >= min_len:
                    # Check if this region is already found (in either direction)
                    new_result = np.array([
                        i - dp[i, j] + 1,  # start1
                        i,             # end1
                        j - dp[i, j] + 1,  # start2
                        j,             # end2
                        dp[i, j]       # length
                    ])

                    # Check if this region is already in results
                    is_duplicate = False
                    for k in range(result_count):
                        if is_same_region(new_result, all_results[k]):
                            is_duplicate = True
                            break

                    if not is_duplicate:
                        all_results[result_count] = new_result
                        result_count += 1
            else:
                dp[i, j] = 0

    # If no results found, return empty array
    if result_count == 0:
        return np.zeros((0, 5), dtype=np.int32)

    # Get only the valid results
    valid_results = all_results[:result_count]

    # Sort by length (descending) using bubble sort
    for i in range(result_count):
        for j in range(0, result_count - i - 1):
            if (valid_results[j][4] < valid_results[j + 1][4] or  # Compare lengths
                (valid_results[j][4] == valid_results[j + 1][4] and  # If lengths equal
                 valid_results[j][0] > valid_results[j + 1][0])):    # Compare start positions
                # Swap rows
                valid_results[j], valid_results[j + 1] = valid_results[j + 1].copy(), valid_results[j].copy()

    # Filter overlapping regions
    used_regions = np.zeros((m, n), dtype=np.bool_)
    filtered_results = np.zeros((result_count, 5), dtype=np.int32)
    filtered_count = 0

    for i in range(result_count):
        result = valid_results[i]
        start1, end1, start2, end2 = int(result[0]), int(result[1]), int(result[2]), int(result[3])

        # Check for overlap
        is_overlap = False
        for x in range(start1 - 1, end1 - 1):
            for y in range(start2 - 1, end2 - 1):
                if used_regions[x, y]:
                    is_overlap = True
                    break
            if is_overlap:
                break

        if not is_overlap:
            # Mark region as used
            for x in range(start1 - 1, end1 - 1):
                for y in range(start2 - 1, end2 - 1):
                    used_regions[x, y] = True

            # Add to filtered results
            filtered_results[filtered_count] = result
            filtered_count += 1

    return filtered_results[:filtered_count]

@njit
def calculate_gc_pairs_numba(primer1_seq: np.array, primer2_seq: np.array) -> int:
    """Calculate number of G-C pairs using Numba"""
    num_gc = 0
    for i in range(len(primer1_seq)):
        if ((primer1_seq[i] == 71 and primer2_seq[i] == 67) or    # G-C
            (primer1_seq[i] == 67 and primer2_seq[i] == 71)):     # C-G
            num_gc += 1
    return num_gc

@njit
def calculate_badness_numba(result, sequence_length: int, primer1: np.array, primer2: np.array) -> float:
    """Calculate badness value using Numba"""
    start1, end1, start2, end2, length = result

    d1 = sequence_length - end1
    d2 = sequence_length - end2

    primer1_seq = primer1[start1 - 1:end1]
    primer2_seq = primer2[start2 - 1:end2]

    num_gc = calculate_gc_pairs_numba(primer1_seq, primer2_seq)

    return (2.0**length * 2.0**num_gc) / ((d1 + 1.0) * (d2 + 1.0))

@njit
def calculate_pair_badness_numba(primer1: np.array, primer2: np.array) -> float:
    """Calculate total badness between two primers using Numba"""
    sequence_length = len(primer1)
    results = find_complementary_regions_numba(primer1, primer2)
    total_badness = 0.0

    for result in results:
        badness_value = calculate_badness_numba(result, sequence_length, primer1, primer2)
        total_badness += badness_value

    return total_badness


def create_badness_matrix(primer_results):

    #  DataFrame
    primer_df = pd.DataFrame([
        {
            "PAIR_ID": pair["pair_id"],
            "LEFT_SEQUENCE": pair["left_primer"]["sequence"],
            "RIGHT_SEQUENCE": pair["right_primer"]["sequence"]
        }
        for pairs in primer_results.values()
        for pair in pairs
    ])

    num_sets = len(primer_df)
    badness_matrix = np.zeros((num_sets, num_sets))

    #  ASCII
    primer_sequences = {}
    for _, row in primer_df.iterrows():
        primer_sequences[f"{row['PAIR_ID']}_LEFT"] = np.array([ord(c) for c in row['LEFT_SEQUENCE']], dtype=np.int32)
        primer_sequences[f"{row['PAIR_ID']}_RIGHT"] = np.array([ord(c) for c in row['RIGHT_SEQUENCE']], dtype=np.int32)

    print(f"\nCreating {num_sets}x{num_sets} badness matrix...")

    #  badness_matrix
    for i in range(num_sets):
        row = primer_df.iloc[i]
        set1_left = primer_sequences[f"{row['PAIR_ID']}_LEFT"]
        set1_right = primer_sequences[f"{row['PAIR_ID']}_RIGHT"]

        for j in range(num_sets):
            row2 = primer_df.iloc[j]
            set2_left = primer_sequences[f"{row2['PAIR_ID']}_LEFT"]
            set2_right = primer_sequences[f"{row2['PAIR_ID']}_RIGHT"]

            # badness
            total_badness = (
                calculate_pair_badness_numba(set1_left, set2_left) +   # Left1-Left2
                calculate_pair_badness_numba(set1_left, set2_right) +  # Left1-Right2
                calculate_pair_badness_numba(set1_right, set2_left) +  # Right1-Left2
                calculate_pair_badness_numba(set1_right, set2_right) + # Right1-Right2
                calculate_pair_badness_numba(set1_left, set1_left) +   # Left1-Left1
                calculate_pair_badness_numba(set1_right, set1_right) + # Right1-Right1
                calculate_pair_badness_numba(set2_left, set2_left) +   # Left2-Left2
                calculate_pair_badness_numba(set2_right, set2_right)   # Right2-Right2
            )
            if i == j:
                total_badness = (calculate_pair_badness_numba(set1_left, set2_left) +
                                calculate_pair_badness_numba(set1_right, set2_right)
                )

            badness_matrix[i, j] = total_badness

        if (i + 1) % 10 == 0:
            print(f"Progress: {((i + 1) / num_sets * 100):.1f}%")

    return badness_matrix

def compute_badness_for_blocks(primer_results: Dict[str, List[Dict]]) -> Dict[str, float]:
    block_badness = {}

    for block_id, primers in primer_results.items():
        if len(primers) < 2:
            print(f"Warning: Block {block_id} has less than 2 primers, skipping badness calculation.")
            block_badness[block_id] = 0
            continue

        block_primer_results = {block_id: primers}
        badness_matrix = create_badness_matrix(block_primer_results)

        total_badness = np.sum(badness_matrix) / 2
        block_badness[block_id] = total_badness

        print(f"Block: {block_id}, Badness Total: {total_badness}")

    return block_badness



def adjust_block_sizes(regions: Dict[int, List[Dict]], block_badness: Dict[str, float], sequence: str, adjustment_size=100, min_product_size=100):
    """
    调整 block 大小，并确保不会超出序列长度，同时避免 250bp 重叠的问题。

    参数：
        - regions: 当前 block 分布
        - block_badness: 计算出的 badness 总值
        - sequence: 原始 DNA 序列
        - adjustment_size: 每次扩展的大小（默认 100bp）
        - min_product_size: 最小 PCR 产物大小，默认 100bp

    返回：
        - 更新后的 `regions`
    """
    sequence_length = len(sequence)  # ✅ **动态计算序列长度**
    mean_badness = np.mean(list(block_badness.values()))

    # 找到 badness 最大的 block
    max_diff_block = max(block_badness, key=lambda k: block_badness[k] - mean_badness)

    print(f"Expanding Block: {max_diff_block}, Current Badness: {block_badness[max_diff_block]}, Mean: {mean_badness}")

    # **提取当前 block 的起始和结束位置**
    parts = max_diff_block.split("_")
    start, end = int(parts[-2]), int(parts[-1])

    # **确保不会超出序列最大长度**
    new_end = min(end + adjustment_size, sequence_length)

    # **新 block ID**
    new_block_id = f"size_1000_region_{start}_{new_end}"

    # **如果扩展后 `new_end == end`，说明已达上限**
    if new_end == end:
        print(f"⚠️ Cannot expand block {max_diff_block} further, reached sequence limit.")
        return regions

    updated_regions = []
    last_block = None

    for window_size, blocks in regions.items():
        new_blocks = []
        for block in blocks:
            if block['id'] == max_diff_block:

                block['end'] = new_end
                block['length'] = new_end - start
                block['id'] = new_block_id
                block['sequence'] = sequence[start:new_end]
            elif block['id'] > max_diff_block:
                temp = block['id'].split('_')
                start1,end1 = int(temp[-2]), int(temp[-1])
                block['end'] = end1 + 100
                block['start'] = start1 + 100
                block['id'] = f"size_1000_region_{start1 + 100}_{end1 + 100}"
                block['sequence'] = sequence[start1 + 100:end1 + 100]

            # ✅ **确保 block 不超出序列长度**
            if block['end'] > sequence_length:
                print(f"⚠️ Trimming block {block['id']} to sequence length ({sequence_length})")
                block['end'] = sequence_length
                block['length'] = sequence_length - block['start']
                block['sequence'] = sequence[block['start']:block['end']]

            # ✅ **合并最后一个小 block**
            if block['end'] == sequence_length:
                if block['length'] < min_product_size:
                    print(f"⚠️ Merging block {block['id']} into previous block due to insufficient length ({block['length']}bp)")
                    last_block['end'] = block['end']
                    last_block['length'] = last_block['end'] - last_block['start']
                    last_block['sequence'] = sequence[last_block['start']:last_block['end']]
                else:
                    new_blocks.append(block)

            else:
                new_blocks.append(block)
                last_block = block

    # for window_size, blocks in regions.items():
    #     new_blocks = []
    #     for block in blocks:
    #         if block['id'] == max_diff_block:
    #             # ✅ **更新 block，移除 250bp 重叠**
    #             block['end'] = new_end
    #             block['length'] = new_end - start
    #             block['id'] = new_block_id
    #             block['sequence'] = sequence[start:new_end]

    #         # ✅ **确保 block 不超出序列长度**
    #         if block['end'] > sequence_length:
    #             print(f"⚠️ Trimming block {block['id']} to sequence length ({sequence_length})")
    #             block['end'] = sequence_length
    #             block['length'] = sequence_length - block['start']
    #             block['sequence'] = sequence[block['start']:block['end']]

    #         # ✅ **合并最后一个小 block**
    #         if block['end'] == sequence_length:
    #             if block['length'] < min_product_size:
    #                 print(f"⚠️ Merging block {block['id']} into previous block due to insufficient length ({block['length']}bp)")
    #                 last_block['end'] = block['end']
    #                 last_block['length'] = last_block['end'] - last_block['start']
    #                 last_block['sequence'] = sequence[last_block['start']:last_block['end']]
    #             else:
    #                 new_blocks.append(block)

    #         else:
    #             new_blocks.append(block)
    #             last_block = block

        updated_regions.append((window_size, new_blocks))

    return dict(updated_regions)




def iterative_block_optimization(sequence: str, regions: Dict[int, List[Dict]], max_deviation=400):
    """
    迭代优化 block，直到最大 badness_total 与均值的差值 <= max_deviation。

    参数：
        - sequence: DNA 序列
        - regions: block 初始分布
        - max_deviation: 最大允许的 badness 偏差，默认 400

    返回：
        - final_result: 经过优化后的引物数据
    """
    iteration = 0
    primer_results = design_primers(regions)  # ✅ 初始引物设计

    while True:
        print(f"\nIteration {iteration + 1}...")

        # **计算每个 block 的 badness**
        block_badness = compute_badness_for_blocks(primer_results)

        # **计算 badness 均值**
        mean_badness = np.mean(list(block_badness.values()))

        # **计算最大偏差**
        max_block = max(block_badness, key=lambda k: abs(block_badness[k] - mean_badness))
        max_deviation_value = abs(block_badness[max_block] - mean_badness)

        print(f"Max Badness Block: {max_block}, Badness Total: {block_badness[max_block]:.2f}")
        print(f"Mean Badness: {mean_badness:.2f}, Max Deviation: {max_deviation_value:.2f}")

        # **判断是否收敛**
        if max_deviation_value <= max_deviation:
            print(f"✅ Max deviation is {max_deviation_value:.2f}, stopping optimization.")
            break

        # **调整 block 结构（只调整 badness_total 最大的 block）**
        regions = adjust_block_sizes(regions, {max_block: block_badness[max_block]}, sequence, adjustment_size=100, min_product_size=100)

        # **重新生成引物**
        primer_results = design_primers(regions)

        iteration += 1

    return primer_results  # ✅ 返回最终优化后的引物数据

if __name__ == "__main__":
    file_path = "/content/drive/MyDrive/ConsensusSequence.fasta"

    try:

        sequence = read_consensus_sequence(file_path)

        regions = sliding_window_regions(sequence, window_sizes=[1000], overlaps=250)

        final_result = iterative_block_optimization(sequence, regions, max_deviation=700)

        print("\n Final Optimized Primer Results:")
        for region, primers in final_result.items():
            print(f"Region: {region}")
            for primer in primers:
                print(f"  {primer}")

    except Exception as e:
        print(f" Error occurred: {str(e)}")

badness_matrix2 = create_badness_matrix(final_result)
print(badness_matrix2)

df_badness = pd.DataFrame(badness_matrix2, index=pair_ids, columns=pair_ids)

print(df_badness.head(10))

df_badness.to_csv("badness_matrix2_with_headers.csv")
print("Saved as badness_matrix2_with_headers.csv")

badness_matrix = create_badness_matrix(primer_results)
print(badness_matrix)

# print(f"Badness matrix shape: {badness_matrix.shape}")  # (行数, 列数)
# print(f"Length of pair_ids: {len(pair_ids)}")  # 期望等于 badness_matrix.shape[0] 和 badness_matrix.shape[1]

### obj
def objective_function(S_init, badness_matrix):
    # function_1
    result = []
    for i in range(badness_matrix.shape[1]):
        max_value = -np.inf
        for s in S_init:
            row_index = s  #
            value = badness_matrix[row_index, i]  #
            if value > max_value:
                max_value = value
        result.append(max_value)
    f1 = sum(result)  # function_1

    # function_2
    total_sum = 0
    n = len(badness_matrix)

    for i in S_init:
        row_index_i = i  #
        for j in S_init:
            col_index_j = j  #
            total_sum += badness_matrix[row_index_i, col_index_j]

    f2 = (1 / n) * total_sum  # function_2

    return f1 - f2


### local search
def fast_local_search(S, S_init, epsilon, badness_matrix):

    k = len(S_init)
    delta = float('inf')

    while delta >= epsilon / k:
        delta = -float('inf')
        best_pa = None
        best_pb = None

        for pa in S_init:

            for pb in S:
                if pb not in S_init:

                    g_pa = objective_function(S_init, badness_matrix)

                    new_S_opt = S_init.copy()
                    new_S_opt.remove(pa)
                    new_S_opt.append(pb)

                    g_pb = objective_function(new_S_opt, badness_matrix)

                    current_delta = g_pa - g_pb

                    # find biggest delta
                    if current_delta > delta:
                        delta = current_delta
                        best_pa = pa
                        best_pb = pb

        if delta >= epsilon / k:
            S_init.remove(best_pa)
            S_init.append(best_pb)
            k = len(S_init)

    return S_init

def generate_S_init(final_result: Dict[str, List[Dict]], df_badness: pd.DataFrame) -> List[str]:

    S_init = []

    for block_id, primer_list in final_result.items():
        max_f1_value = -np.inf
        best_pair_id = None

        # **遍历 block 内的每个 primer 对**
        for primer in primer_list:
            pair_id = primer["pair_id"]

            if pair_id in df_badness.index:
                f1_value = df_badness.loc[pair_id, :].max()

                print(f"Block: {block_id}, Pair ID: {pair_id}, f1 Value: {f1_value}")

                if f1_value > max_f1_value:
                    max_f1_value = f1_value
                    best_pair_id = pair_id  # **记录该 block 贡献 f1 最大的 primer 对 ID**

        if best_pair_id is not None:
            S_init.append(best_pair_id)

    return S_init

S_init = generate_S_init(final_result, df_badness)
print("Generated S_init:", S_init)

####
def fast_local_search(primer_results, S_init, epsilon, badness_df):

    # **从 primer_results 提取所有可能的 pair_id**
    S = [primer["pair_id"] for region in primer_results.values() for primer in region]

    k = len(S_init)
    delta = float('inf')

    print(f"Initial S_init: {S_init}")
    print(f"Candidate S: {S}")

    while delta >= epsilon / k:
        delta = -float('inf')
        best_remove_pair = None
        best_add_pair = None

        # 遍历当前 S_init 的所有 pair_id
        for remove_pair_id in S_init:
            for add_pair_id in S:  # 遍历所有候选对
                if remove_pair_id == add_pair_id:
                    continue  # 跳过相同的 pair

                # 计算当前 S_init 的目标函数 g_pa
                g_pa = objective_function(S_init, badness_df)

                # 替换 remove_pair_id 为 add_pair_id，形成新的 S_opt
                new_S_opt = S_init.copy()
                new_S_opt.remove(remove_pair_id)  # 移除最差的 pair
                new_S_opt.append(add_pair_id)  # 添加新的 pair

                # 计算新的目标函数 g_pb
                g_pb = objective_function(new_S_opt, badness_df)

                # 计算 delta
                current_delta = g_pa - g_pb

                # 找到最大的 delta
                if current_delta > delta:
                    delta = current_delta
                    best_remove_pair = remove_pair_id
                    best_add_pair = add_pair_id

        # 进行替换操作
        if delta >= epsilon / k and best_remove_pair and best_add_pair:
            S_init.remove(best_remove_pair)
            S_init.append(best_add_pair)
            k = len(S_init)

            print(f"Replaced {best_remove_pair} with {best_add_pair}")
            print(f"Updated S_init: {S_init}")

    return S_init

import random

def approximation_algorithm(S, S_init, epsilon, badness_df, primer_results):


    # **Step 1: Run local search on S_init**
    S_opt1 = fast_local_search(S, S_init, epsilon, badness_df, primer_results)
    print(f"S_opt1_output: {S_opt1}")

    # **Step 2: Generate Second Input (S_R and S_opt_R)**
    S_R = [p for p in S if p not in S_opt1]

    # **按 Region 随机选取一个 primer**
    S_opt_R = []
    for region, primers in primer_results.items():
        remaining_primers = [p["pair_id"] for p in primers if p["pair_id"] in S_R]
        if remaining_primers:
            random_pair = random.choice(remaining_primers)
            S_opt_R.append(random_pair)

    print(f"S_opt2_random_input: {S_opt_R}")

    # **Step 3: Run local search on S_opt_R**
    S_opt2 = fast_local_search(S_R, S_opt_R, epsilon, badness_df, primer_results)
    print(f"S_opt2_output: {S_opt2}")

    # **Step 4: Compare Objective Functions**
    f_S_opt1 = objective_function(S_opt1, badness_df)
    f_S_opt2 = objective_function(S_opt2, badness_df)

    return S_opt1 if f_S_opt1 >= f_S_opt2 else S_opt2

print(f"S_init_output:{S_init}")
S = list(range(len(badness_df)))
epsilon = 0.01
final_S_opt = approximation_algorithm(S, S_init, epsilon, badness_df, S_init_m, pair_to_indices)
print(f"final select:{final_S_opt}")